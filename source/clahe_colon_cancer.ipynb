{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tjTb_K5GbGf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os #helps with file directories \n",
        "import math\n",
        "import shutil #Move file from one folder to another\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers.pooling import AvgPool2D\n",
        "from keras.layers import Conv2D,MaxPool2D,Dropout,Flatten,BatchNormalization,GlobalAvgPool2D\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.image import load_img,img_to_array,ImageDataGenerator\n",
        "import keras\n",
        "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
        "from keras.models import load_model\n",
        "import numpy as geek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfFrnU6h0KC9",
        "outputId": "139dbfb2-6fed-4530-bfed-eb2beb6eff49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_6 (Conv2D)           (None, 222, 222, 16)      448       \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 220, 220, 36)      5220      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 110, 110, 36)     0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 108, 108, 64)      20800     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 54, 54, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 52, 52, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 26, 26, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 26, 26, 128)       0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 86528)             0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                5537856   \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,638,245\n",
            "Trainable params: 5,638,245\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model=Sequential()\n",
        "model.add(Conv2D( filters= 16 , kernel_size= (3,3), activation= 'relu', input_shape=(224,224,3) ))\n",
        "\n",
        "model.add(Conv2D(filters=36,kernel_size=(3,3),activation='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(filters=64,kernel_size=(3,3),activation='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(filters=128,kernel_size=(3,3),activation='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "#currently 2 convolutional layer and 2 maxpool layer\n",
        "#convolutional layer is the features of the image\n",
        "#dropping features to avoid overfitting\n",
        "model.add(Dropout(rate=0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "#dense layers\n",
        "model.add(keras.layers.Dense(units=64,activation='relu'))\n",
        "model.add(Dropout(rate=0.25))\n",
        "#Units=1 gives probability of getting one or zero\n",
        "model.add(keras.layers.Dense(units=1,activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNGjeYqE1nxb"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=keras.losses.binary_crossentropy,metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDKicsBO27s1"
      },
      "outputs": [],
      "source": [
        "def preprocessingimages(path):\n",
        "  image_data=ImageDataGenerator(zoom_range=0.2,shear_range=0.2,rescale=1/255,horizontal_flip=True)#data augmentation\n",
        "  image=image_data.flow_from_directory(directory=path,target_size=(224,224),batch_size=32,class_mode='binary')\n",
        "  return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxlZaYNK28lC",
        "outputId": "2223797f-f4e9-4a77-a414-705e5459bda1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 7000 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "path= \"/content/drive/MyDrive/clahe_colon/clahe_colon_new/colon_clahe_train\"\n",
        "train_data=preprocessingimages(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qXQ-3Vd2-xk"
      },
      "outputs": [],
      "source": [
        "#test\n",
        "def preprocessingimages2(path):\n",
        "  image_data=ImageDataGenerator(rescale=1/255)#no augmentation only rescaling\n",
        "  image=image_data.flow_from_directory(directory=path,target_size=(224,224),batch_size=32,class_mode='binary')\n",
        "  return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpRz5xmP3BF9",
        "outputId": "2da2fe5e-5775-4d23-dccd-744f0636893d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1500 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "path= \"/content/drive/MyDrive/clahe_colon/clahe_colon_new/colon_clahe_test\"\n",
        "test_data=preprocessingimages2(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2dKNf5v3EBl"
      },
      "outputs": [],
      "source": [
        "#validation\n",
        "def preprocessingimages3(path):\n",
        "  image_data=ImageDataGenerator(rescale=1/255)#no augmentation only rescaling\n",
        "  image=image_data.flow_from_directory(directory=path,target_size=(224,224),batch_size=32,class_mode='binary')\n",
        "  return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MjQGth73Jsh",
        "outputId": "5f72a945-d4a0-4210-b077-bd8dc01d31bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1500 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "path= \"/content/drive/MyDrive/clahe_colon/clahe_colon_new/colon_clahe_val\"\n",
        "val_data=preprocessingimages3(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4fpkfkN3gW7"
      },
      "outputs": [],
      "source": [
        "es=EarlyStopping(monitor=\"val_accuracy\",min_delta=0.01,patience=10,verbose=1,mode='auto')\n",
        "#patience decides when to stop.Here patience is an issue as it is causing overfitting issues.\n",
        "#difference between accuracy and validation accuracy greater than 10->Overfitting\n",
        "#model checkpoint\n",
        "\n",
        "mc=ModelCheckpoint(monitor=\"val_accuracy\",filepath=\"./testmodel.h5\",verbose=1,save_best_only=True,mode='auto')\n",
        "\n",
        "cd=[es,mc]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psPfBX4u3ifM"
      },
      "outputs": [],
      "source": [
        "h5=model.fit_generator(generator=train_data,\n",
        "                       steps_per_epoch=8,epochs=30,verbose=1,\n",
        "                       validation_data=val_data,validation_steps=16,\n",
        "                       callbacks=cd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ef50CFq69jT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AkRtSim6-rw"
      },
      "source": [
        "VGG\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RH__fQ-65Fg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Flatten,Dense\n",
        "from keras.models import Model,load_model\n",
        "from keras.applications.vgg16 import VGG16,preprocess_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLI4r_iS7vKG",
        "outputId": "2d032f12-3280-4e43-e4cf-b02c29c6954b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "58900480/58889256 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "base_model=VGG16(input_shape=(224,224,3),include_top=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oM6wwLV589mJ",
        "outputId": "27a2ac20-a72a-4b06-d879-ad17372ff6e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<bound method Model.summary of <keras.engine.functional.Functional object at 0x7f84ddff7950>>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for layer in base_model.layers:\n",
        "  layer.trainable=False\n",
        "\n",
        "base_model.summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OG3uYYzW9D7i"
      },
      "outputs": [],
      "source": [
        "#Modifying preprocessing on test data suitable for mobilenet\n",
        "def preprocessingimages4(path):\n",
        "  image_data=ImageDataGenerator(preprocessing_function=preprocess_input)#no augmentation only rescaling\n",
        "  image=image_data.flow_from_directory(directory=path,target_size=(224,224),batch_size=32,class_mode='binary')\n",
        "  return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V671s1ZS9Qs2",
        "outputId": "3e7d19fe-90bc-4253-ca86-6c24b04f4d70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1500 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "path= \"/content/drive/MyDrive/clahe_colon/clahe_colon_new/colon_clahe_test\"\n",
        "ctest_data=preprocessingimages2(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rC7VXzBf9Rrn"
      },
      "outputs": [],
      "source": [
        "def preprocessingimages5(path):\n",
        "  image_data=ImageDataGenerator(preprocessing_function=preprocess_input)#no augmentation only rescaling\n",
        "  image=image_data.flow_from_directory(directory=path,target_size=(224,224),batch_size=32,class_mode='binary')\n",
        "  return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfrv91Bm9iZS",
        "outputId": "92e5b40d-554c-45ab-dd08-a939deae8a66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1500 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "path= \"/content/drive/MyDrive/clahe_colon/clahe_colon_new/colon_clahe_val\"\n",
        "cval_data=preprocessingimages3(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9mj5Id99m7m"
      },
      "outputs": [],
      "source": [
        "def preprocessingimages6(path):\n",
        "  image_data=ImageDataGenerator(zoom_range=0.2,shear_range=0.2,preprocessing_function=preprocess_input,horizontal_flip=True)#data augmentation\n",
        "  image=image_data.flow_from_directory(directory=path,target_size=(224,224),batch_size=32,class_mode='binary')\n",
        "  return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8XVGf1V9pX5",
        "outputId": "a995bcbe-ba1e-4e0f-bd3a-294678c34d65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 7000 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "path= \"/content/drive/MyDrive/clahe_colon/clahe_colon_new/colon_clahe_train\"\n",
        "ctrain_data=preprocessingimages(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqxfEkh--UZ_",
        "outputId": "9f13508f-ec0b-4c0e-93e0-fd9b46e849f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<bound method Model.summary of <keras.engine.functional.Functional object at 0x7f84ddfcdad0>>"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X=Flatten()(base_model.output)\n",
        "X=Dense(units=1,activation='sigmoid')(X)\n",
        "\n",
        "model=Model(base_model.input,X)\n",
        "\n",
        "model.summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6q_NyxEb-YGX"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',loss=keras.losses.binary_crossentropy,metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpiz-Cym-aZi"
      },
      "outputs": [],
      "source": [
        "#callbacks\n",
        "from keras.callbacks import ModelCheckpoint,EarlyStopping \n",
        "mc=ModelCheckpoint(filepath=\"newmodel.h5\",monitor=\"val_accuracy\",verbose=1,save_best_only=True,mode='auto')\n",
        "es=EarlyStopping(monitor=\"val_accuracy\",min_delta=0.01,patience=3,verbose=1)\n",
        "cb=[mc,es]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-oYj8R8-ddW",
        "outputId": "09f1c939-0c89-4f9e-a4a3-0e51917aab0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.8115 - accuracy: 0.5742 \n",
            "Epoch 1: val_accuracy improved from -inf to 0.79688, saving model to newmodel.h5\n",
            "8/8 [==============================] - 358s 48s/step - loss: 0.8115 - accuracy: 0.5742 - val_loss: 0.4148 - val_accuracy: 0.7969\n",
            "Epoch 2/30\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3479 - accuracy: 0.8594 \n",
            "Epoch 2: val_accuracy improved from 0.79688 to 0.91211, saving model to newmodel.h5\n",
            "8/8 [==============================] - 353s 47s/step - loss: 0.3479 - accuracy: 0.8594 - val_loss: 0.2413 - val_accuracy: 0.9121\n",
            "Epoch 3/30\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2367 - accuracy: 0.9180 \n",
            "Epoch 3: val_accuracy improved from 0.91211 to 0.95898, saving model to newmodel.h5\n",
            "8/8 [==============================] - 352s 47s/step - loss: 0.2367 - accuracy: 0.9180 - val_loss: 0.1357 - val_accuracy: 0.9590\n",
            "Epoch 4/30\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.1538 - accuracy: 0.9570 \n",
            "Epoch 4: val_accuracy improved from 0.95898 to 0.96094, saving model to newmodel.h5\n",
            "8/8 [==============================] - 350s 47s/step - loss: 0.1538 - accuracy: 0.9570 - val_loss: 0.1128 - val_accuracy: 0.9609\n",
            "Epoch 5/30\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.1058 - accuracy: 0.9648 \n",
            "Epoch 5: val_accuracy did not improve from 0.96094\n",
            "8/8 [==============================] - 349s 47s/step - loss: 0.1058 - accuracy: 0.9648 - val_loss: 0.1038 - val_accuracy: 0.9570\n",
            "Epoch 6/30\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0916 - accuracy: 0.9688 \n",
            "Epoch 6: val_accuracy improved from 0.96094 to 0.97070, saving model to newmodel.h5\n",
            "8/8 [==============================] - 350s 47s/step - loss: 0.0916 - accuracy: 0.9688 - val_loss: 0.0898 - val_accuracy: 0.9707\n",
            "Epoch 7/30\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0872 - accuracy: 0.9805 \n",
            "Epoch 7: val_accuracy improved from 0.97070 to 0.97656, saving model to newmodel.h5\n",
            "8/8 [==============================] - 347s 47s/step - loss: 0.0872 - accuracy: 0.9805 - val_loss: 0.0695 - val_accuracy: 0.9766\n",
            "Epoch 8/30\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0779 - accuracy: 0.9805 \n",
            "Epoch 8: val_accuracy improved from 0.97656 to 0.98242, saving model to newmodel.h5\n",
            "8/8 [==============================] - 350s 47s/step - loss: 0.0779 - accuracy: 0.9805 - val_loss: 0.0763 - val_accuracy: 0.9824\n",
            "Epoch 9/30\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0572 - accuracy: 0.9961 \n",
            "Epoch 9: val_accuracy did not improve from 0.98242\n",
            "8/8 [==============================] - 349s 47s/step - loss: 0.0572 - accuracy: 0.9961 - val_loss: 0.0756 - val_accuracy: 0.9785\n",
            "Epoch 10/30\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0875 - accuracy: 0.9805 \n",
            "Epoch 10: val_accuracy did not improve from 0.98242\n",
            "8/8 [==============================] - 349s 47s/step - loss: 0.0875 - accuracy: 0.9805 - val_loss: 0.0884 - val_accuracy: 0.9648\n",
            "Epoch 11/30\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9922 \n",
            "Epoch 11: val_accuracy did not improve from 0.98242\n",
            "8/8 [==============================] - 348s 47s/step - loss: 0.0688 - accuracy: 0.9922 - val_loss: 0.0602 - val_accuracy: 0.9766\n",
            "Epoch 11: early stopping\n"
          ]
        }
      ],
      "source": [
        "hist=model.fit_generator(ctrain_data,steps_per_epoch=8,\n",
        "                         epochs=30,validation_steps=16,\n",
        "                         validation_data=cval_data,callbacks=cb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SA8voSp-N0QQ"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "model=load_model(\"/content/newmodel.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyUSYWbAN_JD",
        "outputId": "21353130-d791-443f-f329-a4bc216c0576"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9833333492279053\n"
          ]
        }
      ],
      "source": [
        "acc=model.evaluate_generator(ctest_data)[1]#i is given as it returns loss and accuracy\n",
        "print(acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0l_ovzcOHqr",
        "outputId": "307339f9-c9df-4608-d9aa-2e54af6c9f3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Malignant\n"
          ]
        }
      ],
      "source": [
        "path=\"/content/drive/MyDrive/clahe_colon/clahe_colon_new/colon_clahe_test/colon_clahe_aca/4252.jpg\"\n",
        "img=load_img(path,target_size=(224,224))\n",
        "input_arr=img_to_array(img)/255#dividing the imag by 255 to normalise the image\n",
        "input_arr.shape\n",
        "input_arr=np.expand_dims(input_arr,axis=0)\n",
        "\n",
        "#pred=model.predict_classes(input_arr)[0][0]\n",
        "#pred\n",
        "pred=(model.predict(input_arr)[0][0] > 0.5).astype(\"int32\")\n",
        "if pred:\n",
        "  print(\"Benign\")\n",
        "else:\n",
        "  print(\"Malignant\") "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "clahe-colon cancer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
